---
title: 'Team 5: Project Deliverable'
author: "Roozbeh Jafari, Jeffrey Leung, Phoenix Wang, Ying Wu, Yuzhe Zheng"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup
```{r}
library(data.table)
library(caTools)
library(xgboost)
library(caret)
library(e1071)
library(rpart)
library(lattice)
library(ROCR)
library(pROC)
library(ROSE)
library(randomForest)
library(ggplot2)
library(dplyr)
library(solitude)
library(ggplot2)
library(glmnet)

credit_card_raw = fread("/Users/jeffrey/creditcard.csv")

```

## Exploratory Data Analysis
```{r}
common_theme <- theme(plot.title = element_text(hjust = 0.5, face = "bold"))
ggplot(data = credit_card_raw, aes(x = factor(Class), 
                          y = prop.table(stat(count)), fill = factor(Class),
                          label = scales::percent(prop.table(stat(count))))) +
    geom_bar(position = "dodge") + 
    geom_text(stat = 'count',
              position = position_dodge(.9), 
              vjust = -0.5, 
              size = 3) + 
    scale_x_discrete(labels = c("no fraud", "fraud"))+
    scale_y_continuous(labels = scales::percent)+
    labs(x = 'Class', y = 'Percentage') +
    ggtitle("Distribution of class labels") +
    common_theme
```

Clearly the dataset is very imbalanced with 99.8% of cases being non-fraudulent transactions. A simple measure like accuracy is not appropriate here as even a classifier which labels all transactions as non-fraudulent will have over 99% accuracy. An appropriate measure of model performance here would be AUC (Area Under the Precision-Recall Curve).
```{r}
ggplot(data=credit_card_raw, aes(x = Time, fill = factor(Class))) + geom_histogram(bins = 100)+
labs(x = "Time in seconds since first transaction", y = "No. of transactions") +
ggtitle("Distribution of time of transaction by class") +
facet_grid(Class ~ ., scales = "free_y") + common_theme
```

The ‘Time’ feature looks pretty similar across both types of transactions. One could argue that fraudulent transactions are more uniformly distributed.
```{r}
#histogram
fraud_amount <- credit_card_raw[Class == 1]
ggplot(fraud_amount, aes(as.integer(fraud_amount$Amount))) + geom_histogram() +
labs(x = "Fraud Amount ($ dollars)", y = "Transaction Count") +
ggtitle("Distribution of Fraud Amount") +
facet_grid(Class ~ ., scales = "free_y") + common_theme
```

According to this distribution, the fraud amount is highly skewed to the left.

## Splitting Data
For all the models, we will be using the same 80-20 train-test split. There will be our imbalanced sets which are directly taken from the raw data and downsampled sets that even out the non-fraud to fraud transactions.
```{r}
# Create train and test dataset
credit_card_raw[, test:=0]
credit_card_raw[, "Time":= NULL]
credit_card_raw[sample(nrow(credit_card_raw), 284807*0.2), test:=1]
test <- credit_card_raw[test==1]
train <- credit_card_raw[test==0]
train[, "test" := NULL]
test[, "test" := NULL]
credit_card_raw[, "test" := NULL]

# Convert datatables to dataframes for downsampling
setDF(train)
setDF(test)

# Downsample
set.seed(1)
train$Class <- factor(train$Class)
downsample.train <- downSample(train[, -ncol(train)], train$Class)

test$Class <- factor(test$Class)
downsample.test <- downSample(test[, -ncol(test)], test$Class)
```

## Lasso Regression
For this model, we will be running a lasso regression. We will perform a 5 fold cross validation to determine which lambda value minimizes MSE the most and use that to make predictions. 


We first train a model on our downsampled training dataset.
```{r}
# Create formula
formula <- as.formula(Class ~ .)

# Downsample training set modeling 
downsample.train.matrix <- model.matrix(formula, downsample.train)[, -1]
y.downsample.train <- downsample.train$Class
downsample.fit <- cv.glmnet(downsample.train.matrix, y.downsample.train, family = "binomial", alpha = 1, nfolds = 5)

# Create testing matrices
downsample.test.matrix <- model.matrix(formula, downsample.test) [, -1]
imbalanced.test.matrix <- model.matrix(formula, test)[, -1]
```

Next, we predict on a downsampled testset and the original imbalanced testset.
```{r}
# Predicting Downsample test data
downsample.test.predictions <- predict(downsample.fit, downsample.test.matrix, s = downsample.fit$lambda.min) 
predicted.classes <- ifelse(downsample.test.predictions > 0, 1, 0)
confusionMatrix(as.factor(predicted.classes), downsample.test$Class, positive = "1")
roc.curve(as.numeric(downsample.test$Class), as.numeric(predicted.classes), plotit = TRUE)

# Predicting imbalanced test data
test.predictions <- predict(downsample.fit, imbalanced.test.matrix, s = downsample.fit$lambda.min) 
predicted.classes <- ifelse(test.predictions > 0, 1, 0)
confusionMatrix(as.factor(predicted.classes), test$Class, positive = "1")
roc.curve(as.numeric(test$Class), as.numeric(predicted.classes), plotit = TRUE)
```

Now we train a model on our imbalanced training dataset. Due to the limitations of our computational power, our "imbalanced" set's non-fraud transaction count was reduced to 4000. This would reduce computational strain but still retain the imbalanced aspect of the original dataset. 
```{r}
# Imbalanced training set modeling
class_0 = copy(train[train$Class == 0,])
x <- copy(class_0[sample(nrow(class_0), 4000),])
imbalanced.train <- rbind(x, train[train$Class == 1,])
train.matrix <- model.matrix(formula, imbalanced.train)[, -1]
y.train <- imbalanced.train$Class
imbalanced.fit <- cv.glmnet(train.matrix, y.train, family = "binomial", alpha = 1, nfolds = 5)
```

As we did before, we will predict on a downsampled testset and the original imbalanced test set.
```{r}
# Predicting Downsample test data
downsample.test.predictions <- predict(imbalanced.fit, downsample.test.matrix, s = imbalanced.fit$lambda.min) 
predicted.classes <- ifelse(downsample.test.predictions > 0, 1, 0)
confusionMatrix(as.factor(predicted.classes), downsample.test$Class, positive = "1")
roc.curve(as.numeric(downsample.test$Class), as.numeric(predicted.classes), plotit = TRUE)

#Predicting imbalanced test data
test.predictions <- predict(imbalanced.fit, imbalanced.test.matrix, s = imbalanced.fit$lambda.min) 
predicted.classes <- ifelse(test.predictions > 0, 1, 0)
confusionMatrix(as.factor(predicted.classes), test$Class, positive = "1")
roc.curve(as.numeric(test$Class), as.numeric(predicted.classes), plotit = TRUE)
```

## Logistic Regression
Logistic regression is a simple regression model whose output is a score between 0 and 1. This is achieved by using the logistic function.

Fit logistic regression model by building two models on downsampl(balanced) train data and original(imbalanced) train data, then run each model on both orginal(imbalanced) and downsample(balanced) test data
```{r}
# Fit logistic regression model
set.seed(1)

#fit the model on balanced data(downsampling)
down_fit <- glm(Class ~ ., family = "binomial" , data = downsample.train)
summary(down_fit,)

pred_down <- predict(down_fit, downsample.test) #balanced

#Evaluate model performance on test set
confusionMatrix(data = as.factor(as.numeric(pred_down >0.5)), reference = as.factor(downsample.test$Class), positive = "1")
roc.curve(downsample.test$Class, pred_down, plotit=TRUE)

#predict on imbalanced test set
pred_imbalanced_down <- predict(down_fit, test) #imbalanced
confusionMatrix(data = as.factor(as.numeric(pred_imbalanced_down >0.5)), reference = as.factor(test$Class), positive = "1")
roc.curve(test$Class, pred_imbalanced_down, plotit = TRUE)
```

Apply the model on imbalanced train data(original), fit the model to imbalanced and balanced 
```{r}
org_fit <- glm(Class ~ .,family = "binomial" ,data = train)
summary(org_fit,)

pred_org <- predict(org_fit, downsample.test)

#Evaluate model performance on test set
confusionMatrix(data = as.factor(as.numeric(pred_org >0.5)), reference = as.factor(downsample.test$Class), positive = "1")
roc.curve(downsample.test$Class, pred_org, plotit=TRUE)

pred_imbalanced_org <- predict(org_fit, test)

#Evaluate model performance on test set
confusionMatrix(data = as.factor(as.numeric(pred_imbalanced_org >0.5)), reference = as.factor(test$Class), positive = "1")
roc.curve(test$Class, pred_imbalanced_org, plotit=TRUE)
```

## Decision Tree

Apply 5-folds cross validation to find the best parameter cp for decision tree
```{r}
ctrl <- trainControl(method = "cv", number = 5)
```
Use downsample training set to fit model
```{r}
dt <- train(Class ~ ., data = downsample.train,
               method = 'rpart',
               trControl = ctrl)
```
Find best cp for decision model which is cp = 0.015
Then evaluate the model using downsample test dataset and output the confusion matrix and ROC curve.
```{r}
pred <- predict(dt, downsample.test)

#performance
confusionMatrix(pred, downsample.test$Class, positive = '1')

#ROC curve
roc.curve(downsample.test$Class,pred , plotit=TRUE)
```
Evaluate the model using imbalanced test dataset and output the confusion matrix and ROC curve.
```{r}
pred.imbalanced <- predict(dt, test)

#performance
confusionMatrix(pred.imbalanced, test$Class, positive = '1')

#ROC curve
roc.curve(test$Class, pred.imbalanced, plotit = TRUE)
```
Use imbalanced training set to fit model
```{r}
dt_imbalanced <- train(Class ~ ., data = train,
               method = 'rpart',
               trControl = ctrl)
```
Evaluate the model using downsample test dataset and output the confusion matrix and ROC curve.
```{r}
pred <- predict(dt_imbalanced, downsample.test)

#performance
confusionMatrix(pred, downsample.test$Class, positive = '1')

#ROC curve
roc.curve(downsample.test$Class, pred, plotit=TRUE)
```
Evaluate the model using imbalanced test dataset and output the confusion matrix and ROC curve.
```{r}
#predict on imbalanced test set
pred.imbalanced <- predict(dt_imbalanced, test)

#performance
confusionMatrix(pred.imbalanced, test$Class, positive = '1')

#ROC curve
roc.curve(test$Class, pred.imbalanced, plotit = TRUE)
```

## Random Forest
First Fit the random Forest with the downsampled train dataset(balanced) and plot the feature importance graph. 
```{r}
# Fit random forest model
fit_rndfor <- randomForest(downsample.train$Class~., data=downsample.train, ntree = 500, importance = TRUE)

varImpPlot(fit_rndfor)
```
Then we make predictions by using the downsampled test set and the original test set and compute their confusion matrix. 
```{r}
#make predictions 
pd.test <- predict(fit_rndfor, downsample.test[,-ncol(downsample.test)])
table(observed = downsample.test[,ncol(downsample.test)], predicted = pd.test)

confusionMatrix(pd.test, downsample.test$Class, positive = "1")

pd.test.original <- predict(fit_rndfor, test[,-ncol(test)])
table(observed = test[,ncol(test)], predicted = pd.test.original)

confusionMatrix(pd.test.original, test$Class, positive = "1")
```
We want to see how the model works with the imbalance dataset which is the original train set. 
```{r}
#Random Forest fit with original dataset
fit_rndfor_origin <- randomForest(train$Class~., data=train, ntree = 500, importance = TRUE)
varImpPlot(fit_rndfor_origin)
```
Following with making predictions with both downsampled test set and the original test set and compute the confusion matrix. 
```{r}
#make predictions
pd.test.original2 <- predict(fit_rndfor_origin, test[,-ncol(test)])
table(observed = test[,ncol(test)], predicted = pd.test.original2)

pd.test.original3 <- predict(fit_rndfor_origin, downsample.test[,-ncol(downsample.test)])
table(observed = downsample.test[,ncol(downsample.test)], predicted = pd.test.original3)

confusionMatrix(pd.test.original2, test$Class, positive = "1")
confusionMatrix(pd.test.original3, downsample.test$Class, positive = "1")
```
To better inspect the model accuracy, we also calculate the roc and auc for four models. 
```{r}
#ROC curve and AUC
roc.curve(downsample.test$Class, pd.test, plotit=TRUE)
roc.curve(test$Class, pd.test.original, plotit=TRUE)
roc.curve(test$Class, pd.test.original2, plotit=TRUE)
roc.curve(downsample.test$Class, pd.test.original3, plotit=TRUE)
```

## XGBoost
In order to find the best parameters to fit the XGBoost model, we set randomly chosen values to the parameters and ran k-fold cross-validation. Each time, a set of parameters that maximized AUC was returned. We then created a loop to repeat this process 10 times. We selected the best set of parameters from the 10 iterations and used it to build the XGBoost model.
```{r}
# Cross-validation (downsample.train)
dtrain = data.matrix(downsample.train[,1:29])
best_param = list()
best_seednumber = 1234
best_auc = Inf
best_auc_index = 0

for (iter in 1:10) {
  param <- list(objective = "binary:logistic", eval_metric = "auc")
  cv.nround = 1000
  cv.nfold = 5
  seed.number = sample.int(10000, 1)
  set.seed(seed.number)
  mdcv <- xgb.cv(data=dtrain, params = param,
                 nfold=cv.nfold, nrounds=cv.nround, verbose=0,
                 early_stopping_rounds=8, maximize=TRUE,
                 label=as.numeric(downsample.train$Class)-1)
  
  min_auc = min(mdcv$evaluation_log[, test_auc_mean])
  min_auc_index = which.min(mdcv$evaluation_log[, test_auc_mean])
  
  if (min_auc < best_auc) {
    best_auc = min_auc
    best_auc_index = min_auc_index
    best_seednumber = seed.number
    best_param = param
  }
}

nround = best_auc
set.seed(best_seednumber)
```
We first trained a model on our downsampled training set.
```{r}
# Fit XGBoost model on downsampled training set
xgb = xgboost(data = dtrain,
              params = best_param, 
              nround = nround, 
              label=as.numeric(downsample.train$Class)-1)
```
```{r}
# Feature importance
xgb.importance(model=xgb)
```
Then, we predicted on our downsampled test set and the imbalanced test set.
```{r}
# Apply XGBoost model on downsampled test set
predictions = predict(xgb, data.matrix(downsample.test[,1:29]))
# Transform predictions to binary results
predictions = as.numeric(predictions>0.5)
predictions = as.factor(predictions)
# Confusion matrix
cm1 = confusionMatrix(predictions, downsample.test$Class
                      ,dnn=c("Prediction", "Reference")
                      ,positive='1')
print(cm1)
# Plot ROC curve
roc1 = roc.curve(downsample.test$Class, as.factor(predictions), plotit = TRUE)
print(paste("Area under the curve (AUC):", round(roc1$auc, digits=3)))


# Apply XGBoost model on imbalanced test set
predictions2 = predict(xgb, data.matrix(test[,1:29]))
predictions2 = as.numeric(predictions2>0.5)
predictions2 = as.factor(predictions2)
# Confusion matrix
cm2 = confusionMatrix(predictions2, test$Class
                      ,dnn=c("Prediction", "Reference")
                      ,positive='1')
print(cm2)
# Plot ROC curve
roc2 = roc.curve(test$Class, as.factor(predictions2), plotit = TRUE)
print(paste("Area under the curve (AUC):", round(roc2$auc, digits=3)))
```
Next, we ran k-fold cross-validation to find the optimal parameters like before and used them to train our second model on the imbalanced training set.
```{r}
# Cross-validation (imbalanced training set)
dtrain2 = data.matrix(train[,1:29])
best_param2 = list()
best_seednumber2 = 1234
best_auc2 = Inf
best_auc_index2 = 0

for (iter in 1:10) {
  param <- list(objective = "binary:logistic", eval_metric = "auc")
  cv.nround = 1000
  cv.nfold = 5
  seed.number = sample.int(10000, 1)
  set.seed(seed.number)
  mdcv <- xgb.cv(data=dtrain2, params = param,
                 nfold=cv.nfold, nrounds=cv.nround, verbose=0,
                 early_stopping_rounds=8, maximize=TRUE,
                 label=as.numeric(train$Class)-1)
  
  min_auc = min(mdcv$evaluation_log[, test_auc_mean])
  min_auc_index = which.min(mdcv$evaluation_log[, test_auc_mean])
  
  if (min_auc < best_auc2) {
    best_auc2 = min_auc
    best_auc_index2 = min_auc_index
    best_seednumber2 = seed.number
    best_param2 = param
  }
}

nround2 = best_auc2
set.seed(best_seednumber2)

# Fit XGBoost model on imbalanced training set
xgb2 = xgboost(data = dtrain2,
               params = best_param2, 
               nround = nround2, 
               label=as.numeric(train$Class)-1)
```
```{r}
# Feature importance
xgb.importance(model=xgb2)
```
Again, we predicted on our downsampled test set and the imbalanced test set.
```{r}
# Apply XGBoost model on downsampled test set
pred = predict(xgb2, data.matrix(downsample.test[,1:29]))
pred = as.numeric(pred>0.5)
pred = as.factor(pred)
# Confusion matrix
cm3 = confusionMatrix(pred, downsample.test$Class
                      ,dnn=c("Prediction", "Reference")
                      ,positive='1')
print(cm3)
# Plot ROC curve
roc3 = roc.curve(downsample.test$Class, as.factor(pred), plotit = TRUE)
print(paste("Area under the curve (AUC):", round(roc3$auc, digits=3)))


# Apply XGBoost model on imbalanced test set
pred2 = predict(xgb2, data.matrix(test[,1:29]))
pred2 = as.numeric(pred2>0.5)
pred2 = as.factor(pred2)
# Confusion matrix
cm4 = confusionMatrix(pred2, test$Class
                      ,dnn=c("Prediction", "Reference")
                      ,positive='1')
print(cm4)
# Plot ROC curve
roc4 = roc.curve(test$Class, as.factor(pred2), plotit = TRUE)
print(paste("Area under the curve (AUC):", round(roc4$auc, digits=3)))
```

```{r}
# Model comparison
sets = list("1" = c("downsampled training set", "downsampled test set.")
    ,"2" = c("downsampled training set", "imbalanced test set.")
    ,"3" = c("imbalanced training set", "downsampled test set.")
    ,"4" = c("imbalanced training set", "imbalanced test set."))

i = which.max(c(round(roc1$auc, digits=3)
           ,round(roc2$auc, digits=3)
           ,round(roc3$auc, digits=3)
           ,round(roc4$auc, digits=3)))

cat("Downsampled Training & Downsampled Test"
   ,paste("Sensitivity:"
          , cm1$byClass["Sensitivity"]
          ,"Specificity:"
          , cm1$byClass["Specificity"]
          , " AUC:"
          , round(roc1$auc, digits=3))
   , ""
   ,"Downsampled Training & Imbalanced Test"
   ,paste("Sensitivity:"
          , cm2$byClass["Sensitivity"]
          ,"Specificity:"
          , cm2$byClass["Specificity"]
          , " AUC:"
          , round(roc2$auc, digits=3))
   ,""
   ,"Imbalanced Training & Downsampled Test"
   ,paste("Sensitivity:"
          , cm3$byClass["Sensitivity"]
          ,"Specificity:"
          , cm3$byClass["Specificity"]
          , " AUC:"
          , round(roc3$auc, digits=3))
   ,""
   ,"Imbalanced Training & Imbalanced Test"
   ,paste("Sensitivity:"
          , cm4$byClass["Sensitivity"]
          ,"Specificity:"
          , cm4$byClass["Specificity"]
          , " AUC:"
          , round(roc4$auc, digits=3))
   ,""
   ,paste("As shown above, the model trained on the",
          sets[[i]][1],
          "produced the highest AUC when it was used to predict on the",
          sets[[i]][2])
   ,sep = '\n')
```

## Isolation Forest
As an additional bonus, we wanted to try Isolation Forest. This is an unsupervised model that was developed specifically to detect anomalies. 
```{r}
# Copy new data as to not disturb other models
iforest_train <- copy(train)
iforest_test <- copy(test)
```
```{r}
# initiate an isolation forest
iso <- isolationForest$new(sample_size = length(iforest_train))
# fit for data
iso$fit(iforest_train)
```

Next, we obtain the anomally scores. According to the documentation of this package, scores that are closer to 1 are likely outliers, while if all the scores hover around 0.5, then there is a low likelihood of outliers. 

With this in mind, we set the threshold to 0.6.
```{r}
iforest_scores_train = iso$predict(iforest_train)
iforest_scores_train[order(anomaly_score, decreasing = TRUE)]
iforest_train$predictions <- as.factor(ifelse(iforest_scores_train$anomaly_score >=0.6, 1, 0))

iforest_scores_test = iso$predict(iforest_test)
iforest_scores_test[order(anomaly_score, decreasing = TRUE)]
iforest_test$predictions <- as.factor(ifelse(iforest_scores_test$anomaly_score >=0.6, 1, 0))
```

ROC and AUC results
```{r}
# Confusion Matrix and ROC curve of training data
confusionMatrix(iforest_train$predictions, as.factor(train$Class), positive = "1")
roc.curve(train$Class, iforest_train$predictions, plotit = TRUE)
```
```{r}
# Confusion Matrix and ROC curve of test data
confusionMatrix(iforest_test$predictions, as.factor(test$Class), positive = "1")
roc.curve(test$Class, iforest_test$predictions, plotit = TRUE)
```